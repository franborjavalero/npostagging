{
    "arch": "seq2seq", 
    "hidden_encoder": [64],
    "bidirectional_encoder": true,  
    "hidden_decoder": [64],  
    "unit_type": "lstm_gpu",
    "attention_size": 64,
    "beam_width":1,
    "num_epochs": 50000,
    "batch_size": 256,
    "seed": 8,
    "learning_rate": 0.003,
    "max_grad_norm": 5,
    "max_stopping_step": 5
}